{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e75975d-4b55-411e-b0e2-984afa52b795",
   "metadata": {},
   "source": [
    "## Langchain (prompt template)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c4268b-1416-449f-ac8d-d2863f0d775d",
   "metadata": {},
   "source": [
    "Um prompt template é uma estrutura reutilizável que define como os inputs do usuário serão organizados e formatados antes de serem enviados ao modelo de linguagem. Ele funciona como um \"molde\" para gerar prompts consistentes, dinâmicos e mais fáceis de manter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc3cbb-6668-4557-ab61-06d50beda8ee",
   "metadata": {},
   "source": [
    "### 1. Bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f07568-d85d-4fba-a6a9-1257fd5ef150",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain.prompts.few_shot import FewShotPromptTemplate\n",
    "from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate\n",
    "from langchain.prompts.prompt import PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8868ee7-e195-437e-8f68-ea691761e3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acessando API\n",
    "\n",
    "load_dotenv()\n",
    "secret_key = os.getenv(\"OPEN_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1959ffff-ef78-43a8-a1fa-ae729026ba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecionando o modelo\n",
    "\n",
    "llm = OpenAI(model='gpt-3.5-turbo-instruct',\n",
    "            openai_api_key=secret_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459ec181-56da-4b9e-8136-c7338ccad838",
   "metadata": {},
   "source": [
    "### 2. Prompt template padrão/básico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a8d040-327b-42e6-bb5e-c85201e1c10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template 1\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Responda a seguinte pergunta do usuário:\n",
    "{pergunta}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8b9b06f-9241-4b48-90a0-8c3c3a2c6934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responda a seguinte pergunta do usuário:\n",
      "O que é um SaaS?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(pergunta='O que é um SaaS?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8878ba03-5149-4014-9874-3fbfe5295620",
   "metadata": {},
   "source": [
    "### 3. Prompt template com limitação de saída"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8ae3834-b9a1-4363-8d22-5e9c54f4fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template 2\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "{pergunta}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "499731ac-df4e-43a5-8c6a-8ee53c5f1eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responda a seguinte pergunta do usuário em até 15 palavras:\n",
      "O que é um SaaS?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(pergunta='O que é um SaaS?',n_palavras=15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8a461e-c02c-4287-87e5-27ad35b5141c",
   "metadata": {},
   "source": [
    "Esse prompt template abaixo é uma variação do anterior, mas com um diferencial importante: ele usa partial_variables para pré-definir um dos parâmetros (n_palavras = 10). Usar partial_variables no PromptTemplate é como definir um valor default para uma ou mais variáveis do prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85234e37-61ff-41a7-a0fc-32287dd7a13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template 3\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "Responda a seguinte pergunta do usuário em até {n_palavras} palavras:\n",
    "{pergunta}\n",
    "\"\"\", partial_variables={'n_palavras':10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d1462aa-f607-4686-98a8-5aa9eaf20baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Responda a seguinte pergunta do usuário em até 10 palavras:\n",
      "O que é Langchain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt_template.format(pergunta='O que é Langchain?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1da535-c59c-4283-80ee-725345772dee",
   "metadata": {},
   "source": [
    "### 4. Prompt template com múltiplos prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "892d60ad-4367-44de-aa4d-938657607804",
   "metadata": {},
   "source": [
    "Esse código demonstra como criar vários blocos de prompt reutilizáveis (templates parciais), cada um responsável por uma instrução específica,como limitar palavras, linhas ou definir o idioma da resposta. Depois, esses blocos são concatenados para formar um prompt final completo e parametrizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4602e5c1-f34d-4df0-b0a2-6f70a707a549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template 4 (múltiplos prompts)\n",
    "\n",
    "template_word_count = PromptTemplate.from_template(\"\"\"\n",
    "Responda a pergunta em até {n_palavras} palavras.\n",
    "\"\"\")\n",
    "\n",
    "template_line_count = PromptTemplate.from_template(\"\"\"\n",
    "Responda a pergunta em até {n_linhas} linhas.\n",
    "\"\"\")\n",
    "\n",
    "template_idioma = PromptTemplate.from_template(\"\"\"\n",
    "Retorne a resposta no {idioma}.\n",
    "\"\"\")\n",
    "\n",
    "template_final = (template_word_count + template_line_count + template_idioma + 'Responda a pergunta seguindo as instruções {pergunta}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f573511-e681-4968-8260-d3850caec10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['idioma', 'n_linhas', 'n_palavras', 'pergunta'] template='\\nResponda a pergunta em até {n_palavras} palavras.\\n\\nResponda a pergunta em até {n_linhas} linhas.\\n\\nRetorne a resposta no {idioma}.\\nResponda a pergunta seguindo as instruções {pergunta}'\n"
     ]
    }
   ],
   "source": [
    "print(template_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e36932ac-0c9e-4878-8c60-703ea5d50f18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is the sun?'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_final = template_final.format(n_palavras=15, n_linhas=1, idioma='inglês', pergunta='O que é o sol?')\n",
    "llm.invoke(prompt_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02830819-a4b1-4152-9533-72935fea8180",
   "metadata": {},
   "source": [
    "### 5. Prompt template com prompts encadeados com chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d760b5-6638-4d70-a104-a6e2ce4e20aa",
   "metadata": {},
   "source": [
    "O código abaixo usa ChatPromptTemplate para criar uma conversa estruturada com múltiplas mensagens, separando os papéis de system, human e ai. Ele define o comportamento do assistente, insere mensagens anteriores e recebe uma nova pergunta do usuário, com variáveis como {nome_assistente} e {pergunta} preenchidas dinamicamente. Isso permite simular um chat com contexto e gerar respostas mais naturais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6faf5d6a-1003-43e6-be7d-74823076ebee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Essa é minha dúvida: Quem é você?')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template = ChatPromptTemplate.from_template('Essa é minha dúvida: {duvida}')\n",
    "chat_template.format_messages(duvida='Quem é você?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9dacb5c4-ee20-40cb-aed3-c34a8b6952ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    ('system','Você é um assistente irônico e se chama {nome_assistente}'),\n",
    "    ('human','Olá, como vai?'),\n",
    "    ('ai','Estou bem, como posso ajudar?'),\n",
    "    ('human','{pergunta}')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8c75c88-4250-4921-8a09-d7294f28b676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['nome_assistente', 'pergunta'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['nome_assistente'], template='Você é um assistente irônico e se chama {nome_assistente}')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Olá, como vai?')), AIMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Estou bem, como posso ajudar?')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['pergunta'], template='{pergunta}'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ec3e3fe-1be8-467e-a8a4-0de8f0b65d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Você é um assistente irônico e se chama Botx'),\n",
       " HumanMessage(content='Olá, como vai?'),\n",
       " AIMessage(content='Estou bem, como posso ajudar?'),\n",
       " HumanMessage(content='Qual o seu nome?')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_template.format_messages(nome_assistente='Botx', pergunta='Qual o seu nome?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4171f3d4-6a24-4ab4-9413-aa57ede3d49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29fd93a-9bc9-4268-a10a-06b5e9ad72e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Meu nome é BotX, o assistente irônico. Como posso te ajudar hoje?', response_metadata={'token_usage': {'completion_tokens': 23, 'prompt_tokens': 54, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-b905589c-88e8-4901-a77f-6ca5a626fa61-0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(chat_template.format_messages(nome_assistente='BotX',pergunta='Qual o seu nome?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a368cdd-85a0-41fc-b559-49781f5010dd",
   "metadata": {},
   "source": [
    "### 6. Prompt-few-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420576f1-64c1-49d0-94f0-70b5c3a6589b",
   "metadata": {},
   "source": [
    "O código demonstra o uso de FewShotPromptTemplate no LangChain, onde exemplos anteriores de perguntas e respostas são usados como referência para ensinar ao modelo um padrão de raciocínio passo a passo. Cada exemplo mostra como decompor perguntas complexas em perguntas intermediárias, fornecendo uma estrutura para chegar à resposta final. O modelo então aplica esse mesmo estilo ao responder uma nova pergunta (input='O que Darwin estudou?'), aprendendo a partir dos exemplos fornecidos. Isso é útil para melhorar a qualidade e consistência das respostas, especialmente em tarefas que exigem raciocínio ou decomposição de problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f58ba09-6761-43f6-9a2a-584e539e95e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "exemplos = [\n",
    "    {\"pergunta\": \"Qual é a maior montanha do mundo, o Monte Everest ou o K2?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Qual é a altura do Monte Everest? \n",
    "Resposta intermediária: O Monte Everest tem 8.848 metros de altura. \n",
    "Pergunta de acompanhamento: Qual é a altura do K2? \n",
    "Resposta intermediária: O K2 tem 8.611 metros de altura. \n",
    "Então a resposta final é: Monte Everest \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quando nasceu Charles Darwin? \n",
    "Resposta intermediária: Charles Darwin nasceu em 12 de fevereiro de 1809. \n",
    "Pergunta de acompanhamento: Quando nasceu Albert Einstein? \n",
    "Resposta intermediária: Albert Einstein nasceu em 14 de março de 1879. \n",
    "Então a resposta final é: Charles Darwin \n",
    "\"\"\", \n",
    "    }, \n",
    "    {\"pergunta\": \"Quem foi o pai de Napoleão Bonaparte?\",\n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem foi Napoleão Bonaparte? \n",
    "Resposta intermediária: Napoleão Bonaparte foi um líder militar e imperador francês. \n",
    "Pergunta de acompanhamento: Quem foi o pai de Napoleão Bonaparte? \n",
    "Resposta intermediária: O pai de Napoleão Bonaparte foi Carlo Buonaparte. \n",
    "Então a resposta final é: Carlo Buonaparte \n",
    "\"\"\", \n",
    "    },\n",
    "    {\"pergunta\": \"Os filmes 'O Senhor dos Anéis' e 'O Hobbit' foram dirigidos pelo mesmo diretor?\", \n",
    "     \"resposta\": \n",
    "     \"\"\"São necessárias perguntas de acompanhamento aqui: Sim. \n",
    "Pergunta de acompanhamento: Quem dirigiu 'O Senhor dos Anéis'? \n",
    "Resposta intermediária: 'O Senhor dos Anéis' foi dirigido por Peter Jackson. \n",
    "Pergunta de acompanhamento: Quem dirigiu 'O Hobbit'? \n",
    "Resposta intermediária: 'O Hobbit' também foi dirigido por Peter Jackson. \n",
    "Então a resposta final é: Sim \n",
    "\"\"\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb2745bd-3019-4367-a4a9-47764612e249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pergunta Qual é a maior montanha do mundo, o Monte Everest ou o K2?\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Qual é a altura do Monte Everest? \\nResposta intermediária: O Monte Everest tem 8.848 metros de altura. \\nPergunta de acompanhamento: Qual é a altura do K2? \\nResposta intermediária: O K2 tem 8.611 metros de altura. \\nEntão a resposta final é: Monte Everest \\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_prompt = PromptTemplate(\n",
    "    input_variables=[\"pergunta\", \"resposta\"],\n",
    "    template=\"Pergunta {pergunta}\\n{resposta}\"\n",
    ")\n",
    "\n",
    "example_prompt.format(**exemplos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cf64e7fe-943e-4a2c-a9e0-1df3986d13ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = FewShotPromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt,\n",
    "    suffix=\"Pergunta: {input}\",\n",
    "    input_variables=[\"input\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a79a5ced-6009-47f3-afb2-afc91fb653b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pergunta Qual é a maior montanha do mundo, o Monte Everest ou o K2?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Qual é a altura do Monte Everest? \n",
      "Resposta intermediária: O Monte Everest tem 8.848 metros de altura. \n",
      "Pergunta de acompanhamento: Qual é a altura do K2? \n",
      "Resposta intermediária: O K2 tem 8.611 metros de altura. \n",
      "Então a resposta final é: Monte Everest \n",
      "\n",
      "\n",
      "Pergunta Quem nasceu primeiro, Charles Darwin ou Albert Einstein?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quando nasceu Charles Darwin? \n",
      "Resposta intermediária: Charles Darwin nasceu em 12 de fevereiro de 1809. \n",
      "Pergunta de acompanhamento: Quando nasceu Albert Einstein? \n",
      "Resposta intermediária: Albert Einstein nasceu em 14 de março de 1879. \n",
      "Então a resposta final é: Charles Darwin \n",
      "\n",
      "\n",
      "Pergunta Quem foi o pai de Napoleão Bonaparte?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem foi Napoleão Bonaparte? \n",
      "Resposta intermediária: Napoleão Bonaparte foi um líder militar e imperador francês. \n",
      "Pergunta de acompanhamento: Quem foi o pai de Napoleão Bonaparte? \n",
      "Resposta intermediária: O pai de Napoleão Bonaparte foi Carlo Buonaparte. \n",
      "Então a resposta final é: Carlo Buonaparte \n",
      "\n",
      "\n",
      "Pergunta Os filmes 'O Senhor dos Anéis' e 'O Hobbit' foram dirigidos pelo mesmo diretor?\n",
      "São necessárias perguntas de acompanhamento aqui: Sim. \n",
      "Pergunta de acompanhamento: Quem dirigiu 'O Senhor dos Anéis'? \n",
      "Resposta intermediária: 'O Senhor dos Anéis' foi dirigido por Peter Jackson. \n",
      "Pergunta de acompanhamento: Quem dirigiu 'O Hobbit'? \n",
      "Resposta intermediária: 'O Hobbit' também foi dirigido por Peter Jackson. \n",
      "Então a resposta final é: Sim \n",
      "\n",
      "\n",
      "Pergunta: Quem nasceu primeiro, Nikola Tesla ou Albert Einstein?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(input='Quem nasceu primeiro, Nikola Tesla ou Albert Einstein?'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88566103-d747-4b26-89c9-093240a1ea27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSão necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quando nasceu Nikola Tesla? \\nResposta intermediária: Nikola Tesla nasceu em 10 de julho de 1856. \\nPergunta de acompanhamento: Quando nasceu Albert Einstein? \\nResposta intermediária: Albert Einstein nasceu em 14 de março de 1879. \\nEntão a resposta final é: Nikola Tesla'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(prompt.format(input='Quem nasceu primeiro, Nikola Tesla ou Albert Einstein?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add36da2-2db5-408e-8a40-92f5d88ff307",
   "metadata": {},
   "source": [
    "### 7. Prompt-few-shot em chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa9c0f8-71f0-40f9-8ed7-e7d9a1c452ed",
   "metadata": {},
   "source": [
    "O código abaixo usa o FewShotChatMessagePromptTemplate para criar uma interação de chat com base em exemplos de perguntas e respostas anteriores. Ele define um template de chat com mensagens do tipo human e ai, que servem como exemplos para ensinar o modelo a responder perguntas similares. Após criar o template de exemplo, ele é usado dentro de um ChatPromptTemplate para construir a conversa final, onde uma nova pergunta (\"Qual ginasta feminina no Brasil ganhou mais medalhas?\") é formatada e enviada ao modelo. Esse processo permite ao modelo entender o padrão de respostas esperadas com base em exemplos dados, melhorando a consistência e a qualidade das respostas para perguntas semelhantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "861714e4-ccd0-47ad-8d1b-972effbcccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = ChatOpenAI(api_key=secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be7cb85d-f272-40ab-804a-9cb483e87d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HumanMessage(content='Qual é a maior montanha do mundo, o Monte Everest ou o K2?'), AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Qual é a altura do Monte Everest? \\nResposta intermediária: O Monte Everest tem 8.848 metros de altura. \\nPergunta de acompanhamento: Qual é a altura do K2? \\nResposta intermediária: O K2 tem 8.611 metros de altura. \\nEntão a resposta final é: Monte Everest \\n')]\n"
     ]
    }
   ],
   "source": [
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [(\"human\", \"{pergunta}\"),\n",
    "     (\"ai\", \"{resposta}\")]\n",
    ")\n",
    "\n",
    "print(example_prompt.format_messages(**exemplos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3ed3e169-20b9-4d8a-9cb6-9fc5e5267203",
   "metadata": {},
   "outputs": [],
   "source": [
    "few_shot_template = FewShotChatMessagePromptTemplate(\n",
    "    examples=exemplos,\n",
    "    example_prompt=example_prompt\n",
    ")\n",
    "\n",
    "prompt_final = ChatPromptTemplate.from_messages([\n",
    "    few_shot_template,\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "prompt = prompt_final.format_messages(input=\"Rebeca Andrade ganhou mais medalhas que Daiane dos Santos?\")\n",
    "# few_shot_template.format_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84198775-d8b7-48b4-84f3-94d937ded29b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='São necessárias perguntas de acompanhamento aqui: Sim. \\nPergunta de acompanhamento: Quantas medalhas conquistou Rebeca Andrade? \\nResposta intermediária: Rebeca Andrade conquistou duas medalhas nas Olimpíadas de Tóquio 2020, sendo uma prata e um ouro. \\nPergunta de acompanhamento: Quantas medalhas conquistou Daiane dos Santos? \\nResposta intermediária: Daiane dos Santos conquistou uma medalha de ouro nos Jogos Pan-Americanos de 2003 e uma medalha de ouro nos Jogos Sul-Americanos de 2002. \\nEntão a resposta final é: Não, Daiane dos Santos conquistou mais medalhas do que Rebeca Andrade.', response_metadata={'token_usage': {'completion_tokens': 172, 'prompt_tokens': 523, 'total_tokens': 695, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-64baa72c-43cd-4acc-85f0-2309e01d91fa-0')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.invoke(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
